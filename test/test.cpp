/*
 * Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */


#include "BatchStream.h"
#include "EntropyCalibrator.h"

#include "argsParser.h"
#include "buffers.h"
#include "common.h"
#include "logger.h"

#include "NvCaffeParser.h"
#include "NvInfer.h"
#include <cuda_runtime_api.h>
#include <unordered_map>
#include <cstdlib>
#include <fstream>
#include <iostream>
#include <sstream>
#include <vector>
#include <string>
#include <set>
#include <algorithm>
#include<iostream>

const std::string gSampleName = "TensorRT.test";
using namespace std;


class CaffeTest
{
    template <typename T>
    using SampleUniquePtr = std::unique_ptr<T, samplesCommon::InferDeleter>;

public:
    CaffeTest(const samplesCommon::CaffeSampleParams& params)
        : mParams(params)
    {
    }

  
    //! \brief Builds the network engine
    bool build();
    
    //! \brief Runs the TensorRT inference engine for this sample
    bool infer();
    
    samplesCommon::CaffeSampleParams mParams;//define params for class member function conveniently


private:
    //! \brief Parses a Caffe model and creates a TensorRT networkss
    void getInputOutputNames(); //!< Populates input and output mapping of the network

    bool verifyOutput(float* prob, int count) const;//verify  top 5 correction
   
    bool readPerTensorDynamicRangeValues();//read dynamic range from saved calibration table
    
    bool setDynamicRange(nvinfer1::INetworkDefinition* network);//set the dynamic range for each layed

    void setLayerPrecision(nvinfer1::INetworkDefinition* network);//set each layer precision strictly,if the precision is not supported, tensorrt will choose the fastest completion.
    
    nvinfer1::ICudaEngine* loadEngine(const std::string& engine);//build Engine
    bool saveEngine(ICudaEngine* engine, std::string& fileName);//Load Engine

    nvinfer1::ICudaEngine* mEngine{nullptr}; //!< The TensorRT engine used to run the network
    IRuntime* runtime{nullptr};

    std::map<std::string, std::string> mInOut; //!< Input and output mapping of the network
    
    nvinfer1::Dims mInputDims; //!< The dimensions of the input to the network

    nvinfer1::Dims mOutputDims; //!< The dimensions of the output to the network
    
    std::unordered_map<std::string, float> mPerTensorDynamicRangeMap; //!< Mapping from tensor name to max absolute dynamic range values
  
};
//The fllowing two functions: the absolute maximum dynamic range values generated by the calibrator in calibration table is hexadecimal so the exchange function is required to get float type
int str_2_hex(char* str) {
    int num = 0, i;
    for (i = 0; i < 2; i++) {
        num *= 16;
        if (*str >= '0' && *str <= '9') num += *str - '0';
        else if (*str >= 'a' && *str <= 'f') num += *str - 'a' + 10;
        else if (*str >= 'A' && *str <= 'F') num += *str - 'A' + 10;
        str++;
    }
    return num;
}

void trans(string str,float &range) {
    string a1 = str.substr(6, 2);
    string a2 = str.substr(4, 2);
    string a3 = str.substr(2, 2);
    string a4 = str.substr(0, 2);
    unsigned char temp1 = str_2_hex(&a1[0]);
    unsigned char temp2 = str_2_hex(&a2[0]);
    unsigned char temp3 = str_2_hex(&a3[0]);
    unsigned char temp4 = str_2_hex(&a4[0]);
    unsigned char pMem[4];
    pMem[0] = temp1;
    pMem[1] = temp2;
    pMem[2] = temp3;
    pMem[3] = temp4;
    float* p = (float*)pMem;
    range = *p;
   // return p;
    //unsigned char pMem[]
}

void CaffeTest::getInputOutputNames()
{
    int nbindings = mEngine->getNbBindings();
    assert(nbindings == 2);
    for (int b = 0; b < nbindings; ++b)
    {
        nvinfer1::Dims dims = mEngine->getBindingDimensions(b);
        if (mEngine->bindingIsInput(b))
        {
                gLogInfo << "Found input: "
                         << mEngine->getBindingName(b)
                         << " shape=" << dims
                         << " dtype=" << (int) mEngine->getBindingDataType(b)
                         << std::endl;
            
            mInOut["input"] = mEngine->getBindingName(b);// get the data layer name,for example,data
        }
        else
        {

                gLogInfo << "Found output: "
                         << mEngine->getBindingName(b)
                         << " shape=" << dims
                         << " dtype=" << (int) mEngine->getBindingDataType(b)// get the softmax layer name,for example,prob
                         << std::endl;
            
            mInOut["output"] = mEngine->getBindingName(b);
        }
    }
}

//! \brief Creates the network, configures the builder and creates the network engine
//!
//! \details This function creates the  network by parsing the caffe model and buildsss
//!          the engine that will be used to run  (mEngine)
//!
//! \return Returns true if the engine was created successfully and false otherwise

bool CaffeTest::build()
{
  
   if(!mParams.engine.empty())//load saved engine to reduce the building engine time
   {
      mEngine = loadEngine(mParams.engine);
      
      getInputOutputNames();
    
    
      // derive input/output dims from engine bindings
      const int inputIndex = mEngine->getBindingIndex(mInOut["input"].c_str());
      mInputDims = mEngine->getBindingDimensions(inputIndex);

      const int outputIndex = mEngine->getBindingIndex(mInOut["output"].c_str());
      mOutputDims = mEngine->getBindingDimensions(outputIndex);

      if(! mEngine) return false;
      else return  true;

   }

    IBuilder* builder = createInferBuilder(gLogger);
    if (!builder)
    {
        return false;
    }
    INetworkDefinition* network = builder->createNetwork();
    if (!network)
    {
        return false;
    }
    IBuilderConfig* config = builder->createBuilderConfig();
    if (!config)
    {
        return false;
    }

    nvcaffeparser1::ICaffeParser* parser = nvcaffeparser1::createCaffeParser();
    if (!parser)
    {
        return false;
    }
    //parse the weightsFileName and prototxtFileName
    const nvcaffeparser1::IBlobNameToTensor* blobNameToTensor = parser->parse(locateFile(mParams.prototxtFileName, mParams.dataDirs).c_str(),
            locateFile(mParams.weightsFileName, mParams.dataDirs).c_str(), *network,mParams.int8 ? DataType::kFLOAT : DataType::kHALF);

    for (auto& s : mParams.outputTensorNames)// mark  output tensor name,the softmax lay name:prob
    {
        network->markOutput(*blobNameToTensor->find(s.c_str()));
    }
    std::unique_ptr<IInt8Calibrator> calibrator;//set calibrator to calibrate tensor from FP32/FP16 to int 8

    builder->setMaxBatchSize(128);//set falgs
    config->setAvgTimingIterations(8);
    config->setMinTimingIterations(1);
    config->setMaxWorkspaceSize(1_GiB);
    config->setFlag(BuilderFlag::kGPU_FALLBACK);//For DLA,if the lay computation DLA doesn't support,the computation fall back to GPU
    config->setFlag(BuilderFlag::kDEBUG);

 
    if (mParams.fp16)// set FP16 mode
    {
        config->setFlag(BuilderFlag::kFP16);
    }
    if (mParams.int8)//set int8 mode
    {
        config->setFlag(BuilderFlag::kINT8);
        config->setFlag(BuilderFlag::kSTRICT_TYPES);
     
     if (mParams.calibfile.empty())//if providing calibration file, the way 2 will be used. Otherwise, the way will be used.
      {//For a engine ,the calibration wil be performed once and the calibration table will be saved for future use
        nvinfer1::Dims InputDims{3,3,227,227};

    	PPMBatchStream calibrationStream(50, 10,InputDims);

    	calibrator.reset(new Int8EntropyCalibrator2<PPMBatchStream>( calibrationStream, -1,  mParams.Network.c_str(),
        mParams.inputTensorNames[0].c_str()));
   
        config->setInt8Calibrator(calibrator.get());
      }
     else
     {
         config->setInt8Calibrator(nullptr);
         setLayerPrecision(network);

        if (!setDynamicRange(network))
      {
        gLogError << "Unable to set per tensor dynamic range." << std::endl;
        return false;
      }
     }
    }
    if(mParams.dlaCore>=0)//set DLA falgs,there exists two dla,so the flag can be set at 0 or 1;
    {
    samplesCommon::enableDLA(builder, config, mParams.dlaCore);
    }
     
    mEngine = builder->buildEngineWithConfig(*network, *config);//build engine for inference

    if (!mEngine){
       return false;
    } 

    string device = (mParams.dlaCore == -1 )? "GPU" :((mParams.dlaCore == 0 )? "DLA0" : "DLA1");
    string precision = mParams.int8 ? "INT8":"FP16";
    string str = "/home/peppa3/" + mParams.Network + "engine"+ device + precision;

    if (!saveEngine(mEngine,str)){//save engine
    
      gLogInfo << "failed to save engine " << std::endl;
       return false;
    }
    gLogInfo << "here !!! " << std::endl;
    getInputOutputNames();
    
    
    // derive input/output dims from engine bindings
    const int inputIndex = mEngine->getBindingIndex(mInOut["input"].c_str());
    mInputDims = mEngine->getBindingDimensions(inputIndex);

    const int outputIndex = mEngine->getBindingIndex(mInOut["output"].c_str());
    mOutputDims = mEngine->getBindingDimensions(outputIndex);

    
    network->destroy();
    parser->destroy();
    builder->destroy();

    return true;
}

//!
//! \brief Uses a caffe parser to crsssseate the googlenet Network and marks the s
//!        output layers
//!
//! \param network Pointer to the network that will be populated with the googlenet network
//!
//! \param builder Pointer to the engine builderss
//!


void CaffeTest::setLayerPrecision(nvinfer1::INetworkDefinition* network)
{
    gLogInfo << "Setting Per Layer Computation Precision" << std::endl;
    for (int i = 0; i < network->getNbLayers(); ++i)
    {
        auto layer = network->getLayer(i);
        if (mParams.verbose)
        {
            std::string layerName = layer->getName();
            gLogInfo << "Layer: " << layerName << ". Precision: INT8" << std::endl;
        }
        // set computation precision of the layer
        layer->setPrecision(nvinfer1::DataType::kINT8);

        for (int j = 0; j < layer->getNbOutputs(); ++j)
        {
            std::string tensorName = layer->getOutput(j)->getName();
            if (mParams.verbose)
            {
                std::string tensorName = layer->getOutput(j)->getName();
                gLogInfo << "Tensor: " << tensorName << ". OutputType: INT8" << std::endl;
            }
            // set output type of the tensor
            layer->setOutputType(j, nvinfer1::DataType::kINT8);
        }
    }
}
bool CaffeTest::readPerTensorDynamicRangeValues()//read dynamic range from calibration table
{
    gLogInfo << "strating read " << std::endl;
    std::ifstream iDynamicRangeStream(mParams.calibfile);
    if (!iDynamicRangeStream)
    {
        gLogError << "Could not find per tensor scales file: " << mParams.calibfile << std::endl;
        return false;
    }

    std::string line;
    char delim = ':';
    std::getline(iDynamicRangeStream, line);
    while (std::getline(iDynamicRangeStream, line))
    {
        std::istringstream iline(line);
        std::string token;
        std::getline(iline, token, delim);
        gLogInfo << token ;
        std::string tensorName = token;
        std::getline(iline, token, delim);

        float dynamicRange ;
        trans(token.erase(0,1),dynamicRange);//convert hex to float
     
        gLogInfo <<token<<std::endl;
        gLogInfo<<"  "<<dynamicRange<<std::endl;

        mPerTensorDynamicRangeMap[tensorName] = dynamicRange;
    }
     gLogInfo << "strating sucess " << std::endl;
    if (mParams.verbose)
    {
        gLogInfo << "Per Tensor Dynamic Range Values for the Network:" << std::endl;
        for (auto iter = mPerTensorDynamicRangeMap.begin(); iter != mPerTensorDynamicRangeMap.end(); ++iter)
            gLogInfo << "Tensor: " << iter->first << ". Max Absolute Dynamic Range: " << iter->second << std::endl;
    }
    return true;
}

bool CaffeTest::setDynamicRange(nvinfer1::INetworkDefinition* network)//set dynamic range for each layer
{
    // populate per tensor dynamic range
    if (!readPerTensorDynamicRangeValues())
    {
        return false;
    }

    gLogInfo << "Setting Per Tensor Dynamic Range" << std::endl;
    if (mParams.verbose)
    {
        gLogInfo << "If dynamic range for a tensor is missing, TensorRT will run inference assuming dynamic range for the tensor as optional." << std::endl;
        gLogInfo << "If dynamic range for a tensor is required then inference will fail. Follow README.md to generate missing per tensor dynamic range." << std::endl;
    }

    // set dynamic range for network input tensors
    for (int i = 0; i < network->getNbInputs(); ++i)
    {
        string tName = network->getInput(i)->getName();
        if (mPerTensorDynamicRangeMap.find(tName) != mPerTensorDynamicRangeMap.end())
        {
            network->getInput(i)->setDynamicRange(-mPerTensorDynamicRangeMap.at(tName), mPerTensorDynamicRangeMap.at(tName));
        }
        else
        {
            if (mParams.verbose)
            {
                gLogWarning << "Missing dynamic range for tensor: " << tName << std::endl;
            }
        }
    }//dynamic range :(-absolute range, absolute range)

    // set dynamic range for layer output tensors
    for (int i = 0; i < network->getNbLayers(); ++i)
    {
        for (int j = 0; j < network->getLayer(i)->getNbOutputs(); ++j)
        {
            string tName = network->getLayer(i)->getOutput(j)->getName();
            if (mPerTensorDynamicRangeMap.find(tName) != mPerTensorDynamicRangeMap.end())
            {
                
                network->getLayer(i)->getOutput(j)->setDynamicRange(-mPerTensorDynamicRangeMap.at(tName), mPerTensorDynamicRangeMap.at(tName));
            }
            else
            {
                if (mParams.verbose)
                {
                    gLogWarning << "Missing dynamic range for tensor: " << tName << std::endl;
                }
            }
        }
    }

    if (mParams.verbose)
    {
        gLogInfo << "Per Tensor Dynamic Range Values for the Network:" << std::endl;
        for (auto iter = mPerTensorDynamicRangeMap.begin(); iter != mPerTensorDynamicRangeMap.end(); ++iter)
            gLogInfo << "Tensor: " << iter->first << ". Max Absolute Dynamic Range: " << iter->second << std::endl;
    }
    return true;
}

ICudaEngine* CaffeTest::loadEngine(const std::string& engine)//serialize engine to a plan
{
    std::ifstream engineFile(engine, std::ios::binary);
    if (!engineFile)
    {
        gLogInfo << "Error opening engine file: " << engine << std::endl;
        return nullptr;
    }

    engineFile.seekg(0, engineFile.end);
    long int fsize = engineFile.tellg();
    engineFile.seekg(0, engineFile.beg);

    std::vector<char> engineData(fsize);
    engineFile.read(engineData.data(), fsize);
    if (!engineFile)
    {
        gLogInfo << "Error loading engine file: " << engine << std::endl;
        return nullptr;
    }

    runtime = createInferRuntime(gLogger.getTRTLogger());

    if (mParams.dlaCore != -1)
    {
        runtime->setDLACore(mParams.dlaCore);
    }
    
    return runtime->deserializeCudaEngine(engineData.data(), fsize, nullptr);
}

bool CaffeTest::saveEngine(ICudaEngine* engine, std::string& fileName)//deserialize a plan to an engine
{
    std::ofstream engineFile(fileName, std::ios::binary);

    if (!engineFile)
    {
        gLogInfo << "Cannot open engine file: " << fileName << std::endl;
        return false;
    }

    IHostMemory* serializedEngine = engine->serialize();
    if (serializedEngine == nullptr)
    {
        gLogInfo << "Engine serialization failed" << std::endl;
        return false;
    }

    engineFile.write(static_cast<char*>(serializedEngine->data()), serializedEngine->size());
    serializedEngine->destroy();

    return !engineFile.fail();
}


bool CaffeTest::verifyOutput(float* prob, int count) const//verify top 5 correction
{

    vector<float> output(prob, prob + mOutputDims.d[0] * mParams.batchSize);
    vector<string> groundtruth;

 
    if (!samplesCommon::readReferenceFile(mParams.lableFileName, groundtruth))
    {
        gLogError << "Unable to read reference file: " << mParams.lableFileName << std::endl;
        return false;
    }
    
    int success= 0 ;
    for(int j = 0; j < mParams.batchSize; j++){

    auto ind = samplesCommon::argsort(output.begin()+j* mOutputDims.d[0] ,output.begin()+(j+1)* mOutputDims.d[0],true);

    gLogInfo << " result: Detected:" << std::endl;
       for (int i = 1; i <= 5; ++i)
      {
        if (std::to_string(ind[i-1]).compare(groundtruth[j+count*mParams.batchSize]) == 0 ) { success +=1;}
      }
    }

    std::cout<<"Top 5: "<<(float)success/mParams.batchSize<<std::endl;
    return true;
}

bool CaffeTest::infer()//do tensorrt inference 
{

    void* buffers[2];

    const int inputIndex = mEngine->getBindingIndex(mInOut["input"].c_str());
    const int outputIndex = mEngine->getBindingIndex(mInOut["output"].c_str());

    CHECK(cudaMalloc((void**)&buffers[inputIndex],mParams.batchSize*3*227*227*sizeof(float)));
    CHECK(cudaMalloc((void**)&buffers[outputIndex],mParams.batchSize*1000*sizeof(float)));
    
    IExecutionContext* context = mEngine->createExecutionContext();
    if (!context)
    {
        return false;
    }

    nvinfer1::Dims InputDims{3,3,227,227};//picture size{3,227,227} Another '3' means this is a 3 DimsChannelsHeightWeoght 
    PPMBatchStream pStream(mParams.batchSize, 39,InputDims);
    int count =0;
    float totalTime{0.0f};
    float ms{0.0f};

    float h2dtime{0.0f},d2htime{0.0f}; 
    
    string precision = mParams.int8 ? "INT8":"FP16";

    string name0 = "/home/peppa3/" + mParams.Network + precision + "h2dtime.txt";
    string name1 = "/home/peppa3/" + mParams.Network + precision + "comptime.txt";
    string name2 = "/home/peppa3/" + mParams.Network + precision + "d2htime.txt";
    
    ofstream outputfile0( name0 );
    ofstream outputfile1( name1 );
    ofstream outputfile2( name2 );

    float* prob = new float[mParams.batchSize * 1000];
    while (pStream.next())// Read the input data into the managed buffers
    {  
        assert(mParams.inputTensorNames.size() == 1);

	cudaEvent_t start0, end0;
        CHECK(cudaEventCreate(&start0));
        CHECK(cudaEventCreate(&end0));
        cudaEventRecord(start0, 0);
      
        int buffsize = mParams.int8 ? mParams.batchSize*3*227*227*sizeof(float):mParams.batchSize*3*227*227*sizeof(float)/2;

        CHECK(cudaMemcpy(buffers[inputIndex], pStream.getBatch(), buffsize, cudaMemcpyHostToDevice));
        // Memcpy from host input buffers to device input bufferss

	cudaEventRecord(end0, 0);
        cudaEventSynchronize(end0);
        cudaEventElapsedTime(&h2dtime, start0, end0);
        cudaEventDestroy(start0);
        cudaEventDestroy(end0);
        
	outputfile0<< "batch "<<count<<" h2d time :"<<h2dtime<<" ms"<<std::endl;


        cudaStream_t stream;
        CHECK(cudaStreamCreate(&stream));

        // Use CUDA events to measure inference time
        cudaEvent_t start1, end1;
        CHECK(cudaEventCreateWithFlags(&start1,cudaEventDefault));
        CHECK(cudaEventCreateWithFlags(&end1,cudaEventDefault));
        cudaEventRecord(start1, stream);

        bool status = context->enqueue(mParams.batchSize, buffers, stream, nullptr);
        if (!status)
        {
            return false;
        }

        cudaEventRecord(end1, stream);
        cudaEventSynchronize(end1);
        cudaEventElapsedTime(&ms, start1, end1);
        cudaEventDestroy(start1);
        cudaEventDestroy(end1);

        CHECK(cudaStreamDestroy(stream));

        totalTime += ms;
        gLogInfo << "Processing "<<count<<"batch time is"<<ms<<" ms!" << std::endl;
        outputfile1<< "batch "<<count<<" computation time :"<<ms<<" ms"<<std::endl;

        // Memcpy from device output buffers to host output buffers
        //buffers.copyOutputToHost();

	cudaEvent_t start2, end2;
        CHECK(cudaEventCreate(&start2));
        CHECK(cudaEventCreate(&end2));
        cudaEventRecord(start2, 0);

        CHECK(cudaMemcpy(prob,buffers[outputIndex],mParams.batchSize*1000*sizeof(float), cudaMemcpyDeviceToHost));
	
	cudaEventRecord(end2, 0);
        cudaEventSynchronize(end2);
        cudaEventElapsedTime(&d2htime, start2, end2);
        cudaEventDestroy(start2);
        cudaEventDestroy(end2);

        outputfile2<< "batch "<<count<<" d2h time :"<<d2htime<<" ms"<<std::endl;

        if(!verifyOutput(prob,count))
        {
         gLogInfo << "Processing batches "<<count<<" wrong!" << std::endl;
        }
        count++;
        if (pStream.getBatchesRead() % mParams.batchSize == 0)
        {
            gLogInfo << "Processing next set of max "<<mParams.batchSize<<" batches" << std::endl;
        }
    }

        CHECK(cudaFree(buffers[inputIndex]));
        CHECK(cudaFree(buffers[outputIndex]));

	outputfile0.close();
	outputfile1.close();
	outputfile2.close();

        context->destroy();
	mEngine->destroy();
        runtime->destroy();
        delete[] prob;
       
       gLogInfo << "Processing"<< std::endl;
	return true;
}


// brief Initializes members of the params struct using the command line args

samplesCommon::CaffeSampleParams initializeSampleParams(const samplesCommon::Args& args)
{
    samplesCommon::CaffeSampleParams params;
    if (args.dataDirs.empty())
    {
	params.dataDirs.push_back("data/alexnet/");
    }
    else
    {
        params.dataDirs.push_back(args.dataDirs);
    }

    params.prototxtFileName.assign(args.prototxtFileName);
    params.weightsFileName.assign(args.weightsFileName);
    params.int8 = args.runInInt8;
    params.fp16 = args.runInFp16;
    params.inputTensorNames.push_back("data");
    params.batchSize = args.batch;
    params.outputTensorNames.push_back("prob");
    params.dlaCore = args.useDLACore;
    params.Network.assign(args.Network);
    params.imageFileName = "ILSVRC2012_val_00000";
    params.verbose = true;//print detailed message
    params.lableFileName = locateFile("groundtruth.txt",params.dataDirs);
    params.engine = args.engine;
    params.calibfile = args.calibfile;

    return params;
}


int main(int argc, char** argv)
{
    samplesCommon::Args args;
    bool argsOK = samplesCommon::parseArgs(args, argc, argv);
    if (!argsOK)
    {
        gLogError << "Invalid arguments" << std::endl;
        return EXIT_FAILURE;
    }


    auto sampleTest = gLogger.defineTest(gSampleName, argc, argv);

    gLogger.reportTestStart(sampleTest);

    samplesCommon::CaffeSampleParams params = initializeSampleParams(args);
    CaffeTest sample(params);

    gLogInfo << "Building and running a GPU inference engine for ResNet" << std::endl;

    if (!sample.build())
    {
        return gLogger.reportFail(sampleTest);
    }

    if (!sample.infer())
    {
        return gLogger.reportFail(sampleTest);
    }


    gLogInfo << "Ran " << argv[0] << " with: " << std::endl;

    std::stringstream ss;

    ss << "Input(s): ";
    for (auto& input : sample.mParams.inputTensorNames)
    {
        ss << input << " ";
    }

    gLogInfo << ss.str() << std::endl;

    ss.str(std::string());

    ss << "Output(s): ";
    for (auto& output : sample.mParams.outputTensorNames)
    {
        ss << output << " ";
    }

    gLogInfo << ss.str() << std::endl;

    return gLogger.reportPass(sampleTest);
}
